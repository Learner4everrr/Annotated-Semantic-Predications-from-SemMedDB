{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "choice-seven",
   "metadata": {},
   "source": [
    "# Interview task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "interpreted-glass",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 20\n",
    "MODEL_NAME = \"google-t5/t5-small\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minimal-threat",
   "metadata": {},
   "source": [
    "## check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "liberal-fountain",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "original-tunisia",
   "metadata": {},
   "source": [
    "## Check dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "taken-program",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "thorough-measurement",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('substance_interactions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "light-ready",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PREDICATION_ID</th>\n",
       "      <th>PMID</th>\n",
       "      <th>PREDICATE</th>\n",
       "      <th>INDICATOR_TYPE</th>\n",
       "      <th>PREDICATE_START_INDEX</th>\n",
       "      <th>PREDICATE_END_INDEX</th>\n",
       "      <th>SUBJECT_TEXT</th>\n",
       "      <th>SUBJECT_SEMTYPE</th>\n",
       "      <th>SUBJECT_START_INDEX</th>\n",
       "      <th>SUBJECT_END_INDEX</th>\n",
       "      <th>...</th>\n",
       "      <th>OBJECT_START_INDEX</th>\n",
       "      <th>OBJECT_END_INDEX</th>\n",
       "      <th>OBJECT_SCORE</th>\n",
       "      <th>OBJECT_DIST</th>\n",
       "      <th>OBJECT_MAXDIST</th>\n",
       "      <th>OBJECT_CUI</th>\n",
       "      <th>OBJECT_NOVELTY</th>\n",
       "      <th>TYPE</th>\n",
       "      <th>SENTENCE</th>\n",
       "      <th>LABEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P3100</td>\n",
       "      <td>6499897</td>\n",
       "      <td>INTERACTS_WITH</td>\n",
       "      <td>NOM</td>\n",
       "      <td>1298</td>\n",
       "      <td>1304</td>\n",
       "      <td>SA</td>\n",
       "      <td>orch</td>\n",
       "      <td>1235</td>\n",
       "      <td>1237</td>\n",
       "      <td>...</td>\n",
       "      <td>1329</td>\n",
       "      <td>1332</td>\n",
       "      <td>1000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>C0004057</td>\n",
       "      <td>1</td>\n",
       "      <td>ab</td>\n",
       "      <td>Nor did administration of SA, diflunisal or AS...</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P3101</td>\n",
       "      <td>8369307</td>\n",
       "      <td>INHIBITS</td>\n",
       "      <td>VERB</td>\n",
       "      <td>890</td>\n",
       "      <td>899</td>\n",
       "      <td>rHF</td>\n",
       "      <td>aapp</td>\n",
       "      <td>785</td>\n",
       "      <td>788</td>\n",
       "      <td>...</td>\n",
       "      <td>912</td>\n",
       "      <td>919</td>\n",
       "      <td>888</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>C0242417</td>\n",
       "      <td>1</td>\n",
       "      <td>ab</td>\n",
       "      <td>A       comparative study of recombinant L-cha...</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P3102</td>\n",
       "      <td>3711333</td>\n",
       "      <td>INHIBITS</td>\n",
       "      <td>VERB</td>\n",
       "      <td>1527</td>\n",
       "      <td>1534</td>\n",
       "      <td>alkaloids</td>\n",
       "      <td>orch</td>\n",
       "      <td>1508</td>\n",
       "      <td>1517</td>\n",
       "      <td>...</td>\n",
       "      <td>1541</td>\n",
       "      <td>1550</td>\n",
       "      <td>1000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>C0003805</td>\n",
       "      <td>1</td>\n",
       "      <td>ab</td>\n",
       "      <td>These findings suggest that some nicotinic alk...</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P3103</td>\n",
       "      <td>11742534</td>\n",
       "      <td>INTERACTS_WITH</td>\n",
       "      <td>NOM</td>\n",
       "      <td>746</td>\n",
       "      <td>753</td>\n",
       "      <td>amino acids</td>\n",
       "      <td>aapp</td>\n",
       "      <td>703</td>\n",
       "      <td>714</td>\n",
       "      <td>...</td>\n",
       "      <td>741</td>\n",
       "      <td>745</td>\n",
       "      <td>694</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>C0169658|3716</td>\n",
       "      <td>1</td>\n",
       "      <td>ab</td>\n",
       "      <td>With a       truncated chimaeric IL-5Rbeta-gp1...</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P3104</td>\n",
       "      <td>244385</td>\n",
       "      <td>STIMULATES</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>410</td>\n",
       "      <td>419</td>\n",
       "      <td>Neutral       endopeptidase</td>\n",
       "      <td>aapp</td>\n",
       "      <td>374</td>\n",
       "      <td>401</td>\n",
       "      <td>...</td>\n",
       "      <td>480</td>\n",
       "      <td>491</td>\n",
       "      <td>1000</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>C0039815</td>\n",
       "      <td>1</td>\n",
       "      <td>ab</td>\n",
       "      <td>Neutral       endopeptidase, a zinc-dependent ...</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  PREDICATION_ID      PMID       PREDICATE INDICATOR_TYPE  \\\n",
       "0          P3100   6499897  INTERACTS_WITH            NOM   \n",
       "1          P3101   8369307        INHIBITS           VERB   \n",
       "2          P3102   3711333        INHIBITS           VERB   \n",
       "3          P3103  11742534  INTERACTS_WITH            NOM   \n",
       "4          P3104    244385      STIMULATES            ADJ   \n",
       "\n",
       "   PREDICATE_START_INDEX  PREDICATE_END_INDEX                 SUBJECT_TEXT  \\\n",
       "0                   1298                 1304                           SA   \n",
       "1                    890                  899                          rHF   \n",
       "2                   1527                 1534                    alkaloids   \n",
       "3                    746                  753                  amino acids   \n",
       "4                    410                  419  Neutral       endopeptidase   \n",
       "\n",
       "  SUBJECT_SEMTYPE  SUBJECT_START_INDEX  SUBJECT_END_INDEX  ...  \\\n",
       "0            orch                 1235               1237  ...   \n",
       "1            aapp                  785                788  ...   \n",
       "2            orch                 1508               1517  ...   \n",
       "3            aapp                  703                714  ...   \n",
       "4            aapp                  374                401  ...   \n",
       "\n",
       "   OBJECT_START_INDEX  OBJECT_END_INDEX  OBJECT_SCORE OBJECT_DIST  \\\n",
       "0                1329              1332          1000           2   \n",
       "1                 912               919           888           1   \n",
       "2                1541              1550          1000           1   \n",
       "3                 741               745           694           0   \n",
       "4                 480               491          1000           3   \n",
       "\n",
       "   OBJECT_MAXDIST     OBJECT_CUI OBJECT_NOVELTY  TYPE  \\\n",
       "0               2       C0004057              1    ab   \n",
       "1              15       C0242417              1    ab   \n",
       "2               1       C0003805              1    ab   \n",
       "3               4  C0169658|3716              1    ab   \n",
       "4               5       C0039815              1    ab   \n",
       "\n",
       "                                            SENTENCE  LABEL  \n",
       "0  Nor did administration of SA, diflunisal or AS...      n  \n",
       "1  A       comparative study of recombinant L-cha...      n  \n",
       "2  These findings suggest that some nicotinic alk...      y  \n",
       "3  With a       truncated chimaeric IL-5Rbeta-gp1...      y  \n",
       "4  Neutral       endopeptidase, a zinc-dependent ...      n  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "inside-passing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['PREDICATION_ID', 'PMID', 'PREDICATE', 'INDICATOR_TYPE',\n",
       "       'PREDICATE_START_INDEX', 'PREDICATE_END_INDEX', 'SUBJECT_TEXT',\n",
       "       'SUBJECT_SEMTYPE', 'SUBJECT_START_INDEX', 'SUBJECT_END_INDEX',\n",
       "       'SUBJECT_SCORE', 'SUBJECT_DIST', 'SUBJECT_MAXDIST', 'SUBJECT_CUI',\n",
       "       'SUBJECT_NOVELTY', 'OBJECT_TEXT', 'OBJECT_SEMTYPE',\n",
       "       'OBJECT_START_INDEX', 'OBJECT_END_INDEX', 'OBJECT_SCORE', 'OBJECT_DIST',\n",
       "       'OBJECT_MAXDIST', 'OBJECT_CUI', 'OBJECT_NOVELTY', 'TYPE', 'SENTENCE',\n",
       "       'LABEL'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "quiet-timer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PREDICATION_ID</th>\n",
       "      <th>PMID</th>\n",
       "      <th>PREDICATE</th>\n",
       "      <th>INDICATOR_TYPE</th>\n",
       "      <th>PREDICATE_START_INDEX</th>\n",
       "      <th>PREDICATE_END_INDEX</th>\n",
       "      <th>SUBJECT_TEXT</th>\n",
       "      <th>SUBJECT_SEMTYPE</th>\n",
       "      <th>SUBJECT_START_INDEX</th>\n",
       "      <th>SUBJECT_END_INDEX</th>\n",
       "      <th>...</th>\n",
       "      <th>OBJECT_END_INDEX</th>\n",
       "      <th>OBJECT_SCORE</th>\n",
       "      <th>OBJECT_DIST</th>\n",
       "      <th>OBJECT_MAXDIST</th>\n",
       "      <th>OBJECT_CUI</th>\n",
       "      <th>OBJECT_NOVELTY</th>\n",
       "      <th>TYPE</th>\n",
       "      <th>SENTENCE</th>\n",
       "      <th>LABEL</th>\n",
       "      <th>triple_with_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P3100</td>\n",
       "      <td>6499897</td>\n",
       "      <td>INTERACTS_WITH</td>\n",
       "      <td>NOM</td>\n",
       "      <td>1298</td>\n",
       "      <td>1304</td>\n",
       "      <td>SA</td>\n",
       "      <td>orch</td>\n",
       "      <td>1235</td>\n",
       "      <td>1237</td>\n",
       "      <td>...</td>\n",
       "      <td>1332</td>\n",
       "      <td>1000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>C0004057</td>\n",
       "      <td>1</td>\n",
       "      <td>ab</td>\n",
       "      <td>Nor did administration of SA, diflunisal or AS...</td>\n",
       "      <td>n</td>\n",
       "      <td>Nor did administration of SA, diflunisal or AS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P3101</td>\n",
       "      <td>8369307</td>\n",
       "      <td>INHIBITS</td>\n",
       "      <td>VERB</td>\n",
       "      <td>890</td>\n",
       "      <td>899</td>\n",
       "      <td>rHF</td>\n",
       "      <td>aapp</td>\n",
       "      <td>785</td>\n",
       "      <td>788</td>\n",
       "      <td>...</td>\n",
       "      <td>919</td>\n",
       "      <td>888</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>C0242417</td>\n",
       "      <td>1</td>\n",
       "      <td>ab</td>\n",
       "      <td>A       comparative study of recombinant L-cha...</td>\n",
       "      <td>n</td>\n",
       "      <td>A       comparative study of recombinant L-cha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P3102</td>\n",
       "      <td>3711333</td>\n",
       "      <td>INHIBITS</td>\n",
       "      <td>VERB</td>\n",
       "      <td>1527</td>\n",
       "      <td>1534</td>\n",
       "      <td>alkaloids</td>\n",
       "      <td>orch</td>\n",
       "      <td>1508</td>\n",
       "      <td>1517</td>\n",
       "      <td>...</td>\n",
       "      <td>1550</td>\n",
       "      <td>1000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>C0003805</td>\n",
       "      <td>1</td>\n",
       "      <td>ab</td>\n",
       "      <td>These findings suggest that some nicotinic alk...</td>\n",
       "      <td>y</td>\n",
       "      <td>These findings suggest that some nicotinic alk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P3103</td>\n",
       "      <td>11742534</td>\n",
       "      <td>INTERACTS_WITH</td>\n",
       "      <td>NOM</td>\n",
       "      <td>746</td>\n",
       "      <td>753</td>\n",
       "      <td>amino acids</td>\n",
       "      <td>aapp</td>\n",
       "      <td>703</td>\n",
       "      <td>714</td>\n",
       "      <td>...</td>\n",
       "      <td>745</td>\n",
       "      <td>694</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>C0169658|3716</td>\n",
       "      <td>1</td>\n",
       "      <td>ab</td>\n",
       "      <td>With a       truncated chimaeric IL-5Rbeta-gp1...</td>\n",
       "      <td>y</td>\n",
       "      <td>With a       truncated chimaeric IL-5Rbeta-gp1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P3104</td>\n",
       "      <td>244385</td>\n",
       "      <td>STIMULATES</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>410</td>\n",
       "      <td>419</td>\n",
       "      <td>Neutral       endopeptidase</td>\n",
       "      <td>aapp</td>\n",
       "      <td>374</td>\n",
       "      <td>401</td>\n",
       "      <td>...</td>\n",
       "      <td>491</td>\n",
       "      <td>1000</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>C0039815</td>\n",
       "      <td>1</td>\n",
       "      <td>ab</td>\n",
       "      <td>Neutral       endopeptidase, a zinc-dependent ...</td>\n",
       "      <td>n</td>\n",
       "      <td>Neutral       endopeptidase, a zinc-dependent ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  PREDICATION_ID      PMID       PREDICATE INDICATOR_TYPE  \\\n",
       "0          P3100   6499897  INTERACTS_WITH            NOM   \n",
       "1          P3101   8369307        INHIBITS           VERB   \n",
       "2          P3102   3711333        INHIBITS           VERB   \n",
       "3          P3103  11742534  INTERACTS_WITH            NOM   \n",
       "4          P3104    244385      STIMULATES            ADJ   \n",
       "\n",
       "   PREDICATE_START_INDEX  PREDICATE_END_INDEX                 SUBJECT_TEXT  \\\n",
       "0                   1298                 1304                           SA   \n",
       "1                    890                  899                          rHF   \n",
       "2                   1527                 1534                    alkaloids   \n",
       "3                    746                  753                  amino acids   \n",
       "4                    410                  419  Neutral       endopeptidase   \n",
       "\n",
       "  SUBJECT_SEMTYPE  SUBJECT_START_INDEX  SUBJECT_END_INDEX  ...  \\\n",
       "0            orch                 1235               1237  ...   \n",
       "1            aapp                  785                788  ...   \n",
       "2            orch                 1508               1517  ...   \n",
       "3            aapp                  703                714  ...   \n",
       "4            aapp                  374                401  ...   \n",
       "\n",
       "   OBJECT_END_INDEX  OBJECT_SCORE  OBJECT_DIST OBJECT_MAXDIST     OBJECT_CUI  \\\n",
       "0              1332          1000            2              2       C0004057   \n",
       "1               919           888            1             15       C0242417   \n",
       "2              1550          1000            1              1       C0003805   \n",
       "3               745           694            0              4  C0169658|3716   \n",
       "4               491          1000            3              5       C0039815   \n",
       "\n",
       "  OBJECT_NOVELTY TYPE                                           SENTENCE  \\\n",
       "0              1   ab  Nor did administration of SA, diflunisal or AS...   \n",
       "1              1   ab  A       comparative study of recombinant L-cha...   \n",
       "2              1   ab  These findings suggest that some nicotinic alk...   \n",
       "3              1   ab  With a       truncated chimaeric IL-5Rbeta-gp1...   \n",
       "4              1   ab  Neutral       endopeptidase, a zinc-dependent ...   \n",
       "\n",
       "   LABEL                               triple_with_sentence  \n",
       "0      n  Nor did administration of SA, diflunisal or AS...  \n",
       "1      n  A       comparative study of recombinant L-cha...  \n",
       "2      y  These findings suggest that some nicotinic alk...  \n",
       "3      y  With a       truncated chimaeric IL-5Rbeta-gp1...  \n",
       "4      n  Neutral       endopeptidase, a zinc-dependent ...  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pre_processing(example):\n",
    "    sentence = example['SENTENCE']\n",
    "    subject = example['SUBJECT_TEXT']\n",
    "    object = example['OBJECT_TEXT']\n",
    "    relation = example['PREDICATE']\n",
    "    # text = f\"{subject} [SEP] {relation} [SEP] {object} [SEP] {sentence}\"\n",
    "    text = f\"{sentence} [SEP] {subject} , {relation} , {object}\"\n",
    "    return text\n",
    "\n",
    "df['triple_with_sentence'] = df.apply(pre_processing,axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "military-chosen",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "governing-worth",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/csci5541/zhan8023/.conda/envs/pytorch_jupyter_a40/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Get model's tokenizer.\n",
    "print('Loading tokenizer...')\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "# tokenizer.add_special_tokens({'pad_token': '[PAD]'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expired-rocket",
   "metadata": {},
   "source": [
    "#### test tokenizer for a triple with corresponding sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "israeli-agriculture",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nor did administration of SA, diflunisal or ASA itself impair the       anti-aggregatory effect of a fresh test dose of ASA. [SEP] SA , INTERACTS_WITH , ASA'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = df['triple_with_sentence'].iloc[0]\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "graduate-sarah",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 7005,   410,  3602,    13,  4646,     6,  1227,  6947,    29,   159,\n",
       "           138,    42,     3, 21245,  1402,  4840,  2256,     8,  1181,    18,\n",
       "         31761,   127,    63,  1504,    13,     3,     9,  1434,   794,  6742,\n",
       "            13,     3, 21245,     5,   784,   134,  8569,   908,  4646,     3,\n",
       "             6,     3, 21342, 22034,  4578,   834, 16785,  4611,     3,     6,\n",
       "             3, 21245,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(example, return_tensors='pt', truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "direct-acoustic",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing(example):\n",
    "    res = tokenizer(example['triple_with_sentence'])\n",
    "    # res['label'] = example['LABEL']\n",
    "    res['label'] = 1 if example['LABEL']=='y' else 0\n",
    "    return res\n",
    "df['data'] = df.apply(processing, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "sonic-terrace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [7005, 410, 3602, 13, 4646, 6, 1227, 6947, 29, 159, 138, 42, 3, 21245, 1402, 4840, 2256, 8, 1181, 18, 31761, 127, 63, 1504, 13, 3, 9, 1434, 794, 6742, 13, 3, 21245, 5, 784, 134, 8569, 908, 4646, 3, 6, 3, 21342, 22034, 4578, 834, 16785, 4611, 3, 6, 3, 21245, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'label': 0}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['data'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "august-documentation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['PREDICATION_ID', 'PMID', 'PREDICATE', 'INDICATOR_TYPE',\n",
       "       'PREDICATE_START_INDEX', 'PREDICATE_END_INDEX', 'SUBJECT_TEXT',\n",
       "       'SUBJECT_SEMTYPE', 'SUBJECT_START_INDEX', 'SUBJECT_END_INDEX',\n",
       "       'SUBJECT_SCORE', 'SUBJECT_DIST', 'SUBJECT_MAXDIST', 'SUBJECT_CUI',\n",
       "       'SUBJECT_NOVELTY', 'OBJECT_TEXT', 'OBJECT_SEMTYPE',\n",
       "       'OBJECT_START_INDEX', 'OBJECT_END_INDEX', 'OBJECT_SCORE', 'OBJECT_DIST',\n",
       "       'OBJECT_MAXDIST', 'OBJECT_CUI', 'OBJECT_NOVELTY', 'TYPE', 'SENTENCE',\n",
       "       'LABEL', 'triple_with_sentence', 'data'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "based-suggestion",
   "metadata": {},
   "source": [
    "## split the data, training set 70%, validation set 15%, test set 15%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "numerical-period",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "train_data, test_data = train_test_split(df, test_size=0.3, random_state=RANDOM_STATE)\n",
    "val_data, test_data = train_test_split(test_data, test_size=0.5, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "looking-canada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2100 450 450\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data),len(val_data),len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "published-kidney",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.reset_index()\n",
    "val_data = val_data.reset_index()\n",
    "test_data = test_data.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "alone-increase",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "# import evaluate\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "# accuracy = evaluate.load('accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "special-mitchell",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "\t    predictions, labels = eval_pred\n",
    "\t    predictions = np.argmax(predictions, axis=1)\n",
    "\t    \n",
    "\t    # Calculate precision, recall, and F1 score\n",
    "\t    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='binary')\n",
    "\t    \n",
    "\t    return {\n",
    "\t        'accuracy': accuracy_score(labels, predictions),\n",
    "\t        'precision': precision,\n",
    "\t        'recall': recall,\n",
    "\t        'f1': f1\n",
    "\t    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "empirical-annex",
   "metadata": {},
   "source": [
    "## BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "endless-museum",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type t5 to instantiate a model of type ctrl. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id2label: {0: 'n', 1: 'y'}\n",
      "label2id: {'n': 0, 'y': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CTRLForSequenceClassification were not initialized from the model checkpoint at google-t5/t5-small and are newly initialized: ['classifier.weight', 'h.0.ffn.0.bias', 'h.0.ffn.0.weight', 'h.0.ffn.2.bias', 'h.0.ffn.2.weight', 'h.0.layernorm1.bias', 'h.0.layernorm1.weight', 'h.0.layernorm2.bias', 'h.0.layernorm2.weight', 'h.0.multi_head_attention.Wk.bias', 'h.0.multi_head_attention.Wk.weight', 'h.0.multi_head_attention.Wq.bias', 'h.0.multi_head_attention.Wq.weight', 'h.0.multi_head_attention.Wv.bias', 'h.0.multi_head_attention.Wv.weight', 'h.0.multi_head_attention.dense.bias', 'h.0.multi_head_attention.dense.weight', 'h.1.ffn.0.bias', 'h.1.ffn.0.weight', 'h.1.ffn.2.bias', 'h.1.ffn.2.weight', 'h.1.layernorm1.bias', 'h.1.layernorm1.weight', 'h.1.layernorm2.bias', 'h.1.layernorm2.weight', 'h.1.multi_head_attention.Wk.bias', 'h.1.multi_head_attention.Wk.weight', 'h.1.multi_head_attention.Wq.bias', 'h.1.multi_head_attention.Wq.weight', 'h.1.multi_head_attention.Wv.bias', 'h.1.multi_head_attention.Wv.weight', 'h.1.multi_head_attention.dense.bias', 'h.1.multi_head_attention.dense.weight', 'h.10.ffn.0.bias', 'h.10.ffn.0.weight', 'h.10.ffn.2.bias', 'h.10.ffn.2.weight', 'h.10.layernorm1.bias', 'h.10.layernorm1.weight', 'h.10.layernorm2.bias', 'h.10.layernorm2.weight', 'h.10.multi_head_attention.Wk.bias', 'h.10.multi_head_attention.Wk.weight', 'h.10.multi_head_attention.Wq.bias', 'h.10.multi_head_attention.Wq.weight', 'h.10.multi_head_attention.Wv.bias', 'h.10.multi_head_attention.Wv.weight', 'h.10.multi_head_attention.dense.bias', 'h.10.multi_head_attention.dense.weight', 'h.11.ffn.0.bias', 'h.11.ffn.0.weight', 'h.11.ffn.2.bias', 'h.11.ffn.2.weight', 'h.11.layernorm1.bias', 'h.11.layernorm1.weight', 'h.11.layernorm2.bias', 'h.11.layernorm2.weight', 'h.11.multi_head_attention.Wk.bias', 'h.11.multi_head_attention.Wk.weight', 'h.11.multi_head_attention.Wq.bias', 'h.11.multi_head_attention.Wq.weight', 'h.11.multi_head_attention.Wv.bias', 'h.11.multi_head_attention.Wv.weight', 'h.11.multi_head_attention.dense.bias', 'h.11.multi_head_attention.dense.weight', 'h.12.ffn.0.bias', 'h.12.ffn.0.weight', 'h.12.ffn.2.bias', 'h.12.ffn.2.weight', 'h.12.layernorm1.bias', 'h.12.layernorm1.weight', 'h.12.layernorm2.bias', 'h.12.layernorm2.weight', 'h.12.multi_head_attention.Wk.bias', 'h.12.multi_head_attention.Wk.weight', 'h.12.multi_head_attention.Wq.bias', 'h.12.multi_head_attention.Wq.weight', 'h.12.multi_head_attention.Wv.bias', 'h.12.multi_head_attention.Wv.weight', 'h.12.multi_head_attention.dense.bias', 'h.12.multi_head_attention.dense.weight', 'h.13.ffn.0.bias', 'h.13.ffn.0.weight', 'h.13.ffn.2.bias', 'h.13.ffn.2.weight', 'h.13.layernorm1.bias', 'h.13.layernorm1.weight', 'h.13.layernorm2.bias', 'h.13.layernorm2.weight', 'h.13.multi_head_attention.Wk.bias', 'h.13.multi_head_attention.Wk.weight', 'h.13.multi_head_attention.Wq.bias', 'h.13.multi_head_attention.Wq.weight', 'h.13.multi_head_attention.Wv.bias', 'h.13.multi_head_attention.Wv.weight', 'h.13.multi_head_attention.dense.bias', 'h.13.multi_head_attention.dense.weight', 'h.14.ffn.0.bias', 'h.14.ffn.0.weight', 'h.14.ffn.2.bias', 'h.14.ffn.2.weight', 'h.14.layernorm1.bias', 'h.14.layernorm1.weight', 'h.14.layernorm2.bias', 'h.14.layernorm2.weight', 'h.14.multi_head_attention.Wk.bias', 'h.14.multi_head_attention.Wk.weight', 'h.14.multi_head_attention.Wq.bias', 'h.14.multi_head_attention.Wq.weight', 'h.14.multi_head_attention.Wv.bias', 'h.14.multi_head_attention.Wv.weight', 'h.14.multi_head_attention.dense.bias', 'h.14.multi_head_attention.dense.weight', 'h.15.ffn.0.bias', 'h.15.ffn.0.weight', 'h.15.ffn.2.bias', 'h.15.ffn.2.weight', 'h.15.layernorm1.bias', 'h.15.layernorm1.weight', 'h.15.layernorm2.bias', 'h.15.layernorm2.weight', 'h.15.multi_head_attention.Wk.bias', 'h.15.multi_head_attention.Wk.weight', 'h.15.multi_head_attention.Wq.bias', 'h.15.multi_head_attention.Wq.weight', 'h.15.multi_head_attention.Wv.bias', 'h.15.multi_head_attention.Wv.weight', 'h.15.multi_head_attention.dense.bias', 'h.15.multi_head_attention.dense.weight', 'h.16.ffn.0.bias', 'h.16.ffn.0.weight', 'h.16.ffn.2.bias', 'h.16.ffn.2.weight', 'h.16.layernorm1.bias', 'h.16.layernorm1.weight', 'h.16.layernorm2.bias', 'h.16.layernorm2.weight', 'h.16.multi_head_attention.Wk.bias', 'h.16.multi_head_attention.Wk.weight', 'h.16.multi_head_attention.Wq.bias', 'h.16.multi_head_attention.Wq.weight', 'h.16.multi_head_attention.Wv.bias', 'h.16.multi_head_attention.Wv.weight', 'h.16.multi_head_attention.dense.bias', 'h.16.multi_head_attention.dense.weight', 'h.17.ffn.0.bias', 'h.17.ffn.0.weight', 'h.17.ffn.2.bias', 'h.17.ffn.2.weight', 'h.17.layernorm1.bias', 'h.17.layernorm1.weight', 'h.17.layernorm2.bias', 'h.17.layernorm2.weight', 'h.17.multi_head_attention.Wk.bias', 'h.17.multi_head_attention.Wk.weight', 'h.17.multi_head_attention.Wq.bias', 'h.17.multi_head_attention.Wq.weight', 'h.17.multi_head_attention.Wv.bias', 'h.17.multi_head_attention.Wv.weight', 'h.17.multi_head_attention.dense.bias', 'h.17.multi_head_attention.dense.weight', 'h.18.ffn.0.bias', 'h.18.ffn.0.weight', 'h.18.ffn.2.bias', 'h.18.ffn.2.weight', 'h.18.layernorm1.bias', 'h.18.layernorm1.weight', 'h.18.layernorm2.bias', 'h.18.layernorm2.weight', 'h.18.multi_head_attention.Wk.bias', 'h.18.multi_head_attention.Wk.weight', 'h.18.multi_head_attention.Wq.bias', 'h.18.multi_head_attention.Wq.weight', 'h.18.multi_head_attention.Wv.bias', 'h.18.multi_head_attention.Wv.weight', 'h.18.multi_head_attention.dense.bias', 'h.18.multi_head_attention.dense.weight', 'h.19.ffn.0.bias', 'h.19.ffn.0.weight', 'h.19.ffn.2.bias', 'h.19.ffn.2.weight', 'h.19.layernorm1.bias', 'h.19.layernorm1.weight', 'h.19.layernorm2.bias', 'h.19.layernorm2.weight', 'h.19.multi_head_attention.Wk.bias', 'h.19.multi_head_attention.Wk.weight', 'h.19.multi_head_attention.Wq.bias', 'h.19.multi_head_attention.Wq.weight', 'h.19.multi_head_attention.Wv.bias', 'h.19.multi_head_attention.Wv.weight', 'h.19.multi_head_attention.dense.bias', 'h.19.multi_head_attention.dense.weight', 'h.2.ffn.0.bias', 'h.2.ffn.0.weight', 'h.2.ffn.2.bias', 'h.2.ffn.2.weight', 'h.2.layernorm1.bias', 'h.2.layernorm1.weight', 'h.2.layernorm2.bias', 'h.2.layernorm2.weight', 'h.2.multi_head_attention.Wk.bias', 'h.2.multi_head_attention.Wk.weight', 'h.2.multi_head_attention.Wq.bias', 'h.2.multi_head_attention.Wq.weight', 'h.2.multi_head_attention.Wv.bias', 'h.2.multi_head_attention.Wv.weight', 'h.2.multi_head_attention.dense.bias', 'h.2.multi_head_attention.dense.weight', 'h.20.ffn.0.bias', 'h.20.ffn.0.weight', 'h.20.ffn.2.bias', 'h.20.ffn.2.weight', 'h.20.layernorm1.bias', 'h.20.layernorm1.weight', 'h.20.layernorm2.bias', 'h.20.layernorm2.weight', 'h.20.multi_head_attention.Wk.bias', 'h.20.multi_head_attention.Wk.weight', 'h.20.multi_head_attention.Wq.bias', 'h.20.multi_head_attention.Wq.weight', 'h.20.multi_head_attention.Wv.bias', 'h.20.multi_head_attention.Wv.weight', 'h.20.multi_head_attention.dense.bias', 'h.20.multi_head_attention.dense.weight', 'h.21.ffn.0.bias', 'h.21.ffn.0.weight', 'h.21.ffn.2.bias', 'h.21.ffn.2.weight', 'h.21.layernorm1.bias', 'h.21.layernorm1.weight', 'h.21.layernorm2.bias', 'h.21.layernorm2.weight', 'h.21.multi_head_attention.Wk.bias', 'h.21.multi_head_attention.Wk.weight', 'h.21.multi_head_attention.Wq.bias', 'h.21.multi_head_attention.Wq.weight', 'h.21.multi_head_attention.Wv.bias', 'h.21.multi_head_attention.Wv.weight', 'h.21.multi_head_attention.dense.bias', 'h.21.multi_head_attention.dense.weight', 'h.22.ffn.0.bias', 'h.22.ffn.0.weight', 'h.22.ffn.2.bias', 'h.22.ffn.2.weight', 'h.22.layernorm1.bias', 'h.22.layernorm1.weight', 'h.22.layernorm2.bias', 'h.22.layernorm2.weight', 'h.22.multi_head_attention.Wk.bias', 'h.22.multi_head_attention.Wk.weight', 'h.22.multi_head_attention.Wq.bias', 'h.22.multi_head_attention.Wq.weight', 'h.22.multi_head_attention.Wv.bias', 'h.22.multi_head_attention.Wv.weight', 'h.22.multi_head_attention.dense.bias', 'h.22.multi_head_attention.dense.weight', 'h.23.ffn.0.bias', 'h.23.ffn.0.weight', 'h.23.ffn.2.bias', 'h.23.ffn.2.weight', 'h.23.layernorm1.bias', 'h.23.layernorm1.weight', 'h.23.layernorm2.bias', 'h.23.layernorm2.weight', 'h.23.multi_head_attention.Wk.bias', 'h.23.multi_head_attention.Wk.weight', 'h.23.multi_head_attention.Wq.bias', 'h.23.multi_head_attention.Wq.weight', 'h.23.multi_head_attention.Wv.bias', 'h.23.multi_head_attention.Wv.weight', 'h.23.multi_head_attention.dense.bias', 'h.23.multi_head_attention.dense.weight', 'h.24.ffn.0.bias', 'h.24.ffn.0.weight', 'h.24.ffn.2.bias', 'h.24.ffn.2.weight', 'h.24.layernorm1.bias', 'h.24.layernorm1.weight', 'h.24.layernorm2.bias', 'h.24.layernorm2.weight', 'h.24.multi_head_attention.Wk.bias', 'h.24.multi_head_attention.Wk.weight', 'h.24.multi_head_attention.Wq.bias', 'h.24.multi_head_attention.Wq.weight', 'h.24.multi_head_attention.Wv.bias', 'h.24.multi_head_attention.Wv.weight', 'h.24.multi_head_attention.dense.bias', 'h.24.multi_head_attention.dense.weight', 'h.25.ffn.0.bias', 'h.25.ffn.0.weight', 'h.25.ffn.2.bias', 'h.25.ffn.2.weight', 'h.25.layernorm1.bias', 'h.25.layernorm1.weight', 'h.25.layernorm2.bias', 'h.25.layernorm2.weight', 'h.25.multi_head_attention.Wk.bias', 'h.25.multi_head_attention.Wk.weight', 'h.25.multi_head_attention.Wq.bias', 'h.25.multi_head_attention.Wq.weight', 'h.25.multi_head_attention.Wv.bias', 'h.25.multi_head_attention.Wv.weight', 'h.25.multi_head_attention.dense.bias', 'h.25.multi_head_attention.dense.weight', 'h.26.ffn.0.bias', 'h.26.ffn.0.weight', 'h.26.ffn.2.bias', 'h.26.ffn.2.weight', 'h.26.layernorm1.bias', 'h.26.layernorm1.weight', 'h.26.layernorm2.bias', 'h.26.layernorm2.weight', 'h.26.multi_head_attention.Wk.bias', 'h.26.multi_head_attention.Wk.weight', 'h.26.multi_head_attention.Wq.bias', 'h.26.multi_head_attention.Wq.weight', 'h.26.multi_head_attention.Wv.bias', 'h.26.multi_head_attention.Wv.weight', 'h.26.multi_head_attention.dense.bias', 'h.26.multi_head_attention.dense.weight', 'h.27.ffn.0.bias', 'h.27.ffn.0.weight', 'h.27.ffn.2.bias', 'h.27.ffn.2.weight', 'h.27.layernorm1.bias', 'h.27.layernorm1.weight', 'h.27.layernorm2.bias', 'h.27.layernorm2.weight', 'h.27.multi_head_attention.Wk.bias', 'h.27.multi_head_attention.Wk.weight', 'h.27.multi_head_attention.Wq.bias', 'h.27.multi_head_attention.Wq.weight', 'h.27.multi_head_attention.Wv.bias', 'h.27.multi_head_attention.Wv.weight', 'h.27.multi_head_attention.dense.bias', 'h.27.multi_head_attention.dense.weight', 'h.28.ffn.0.bias', 'h.28.ffn.0.weight', 'h.28.ffn.2.bias', 'h.28.ffn.2.weight', 'h.28.layernorm1.bias', 'h.28.layernorm1.weight', 'h.28.layernorm2.bias', 'h.28.layernorm2.weight', 'h.28.multi_head_attention.Wk.bias', 'h.28.multi_head_attention.Wk.weight', 'h.28.multi_head_attention.Wq.bias', 'h.28.multi_head_attention.Wq.weight', 'h.28.multi_head_attention.Wv.bias', 'h.28.multi_head_attention.Wv.weight', 'h.28.multi_head_attention.dense.bias', 'h.28.multi_head_attention.dense.weight', 'h.29.ffn.0.bias', 'h.29.ffn.0.weight', 'h.29.ffn.2.bias', 'h.29.ffn.2.weight', 'h.29.layernorm1.bias', 'h.29.layernorm1.weight', 'h.29.layernorm2.bias', 'h.29.layernorm2.weight', 'h.29.multi_head_attention.Wk.bias', 'h.29.multi_head_attention.Wk.weight', 'h.29.multi_head_attention.Wq.bias', 'h.29.multi_head_attention.Wq.weight', 'h.29.multi_head_attention.Wv.bias', 'h.29.multi_head_attention.Wv.weight', 'h.29.multi_head_attention.dense.bias', 'h.29.multi_head_attention.dense.weight', 'h.3.ffn.0.bias', 'h.3.ffn.0.weight', 'h.3.ffn.2.bias', 'h.3.ffn.2.weight', 'h.3.layernorm1.bias', 'h.3.layernorm1.weight', 'h.3.layernorm2.bias', 'h.3.layernorm2.weight', 'h.3.multi_head_attention.Wk.bias', 'h.3.multi_head_attention.Wk.weight', 'h.3.multi_head_attention.Wq.bias', 'h.3.multi_head_attention.Wq.weight', 'h.3.multi_head_attention.Wv.bias', 'h.3.multi_head_attention.Wv.weight', 'h.3.multi_head_attention.dense.bias', 'h.3.multi_head_attention.dense.weight', 'h.30.ffn.0.bias', 'h.30.ffn.0.weight', 'h.30.ffn.2.bias', 'h.30.ffn.2.weight', 'h.30.layernorm1.bias', 'h.30.layernorm1.weight', 'h.30.layernorm2.bias', 'h.30.layernorm2.weight', 'h.30.multi_head_attention.Wk.bias', 'h.30.multi_head_attention.Wk.weight', 'h.30.multi_head_attention.Wq.bias', 'h.30.multi_head_attention.Wq.weight', 'h.30.multi_head_attention.Wv.bias', 'h.30.multi_head_attention.Wv.weight', 'h.30.multi_head_attention.dense.bias', 'h.30.multi_head_attention.dense.weight', 'h.31.ffn.0.bias', 'h.31.ffn.0.weight', 'h.31.ffn.2.bias', 'h.31.ffn.2.weight', 'h.31.layernorm1.bias', 'h.31.layernorm1.weight', 'h.31.layernorm2.bias', 'h.31.layernorm2.weight', 'h.31.multi_head_attention.Wk.bias', 'h.31.multi_head_attention.Wk.weight', 'h.31.multi_head_attention.Wq.bias', 'h.31.multi_head_attention.Wq.weight', 'h.31.multi_head_attention.Wv.bias', 'h.31.multi_head_attention.Wv.weight', 'h.31.multi_head_attention.dense.bias', 'h.31.multi_head_attention.dense.weight', 'h.32.ffn.0.bias', 'h.32.ffn.0.weight', 'h.32.ffn.2.bias', 'h.32.ffn.2.weight', 'h.32.layernorm1.bias', 'h.32.layernorm1.weight', 'h.32.layernorm2.bias', 'h.32.layernorm2.weight', 'h.32.multi_head_attention.Wk.bias', 'h.32.multi_head_attention.Wk.weight', 'h.32.multi_head_attention.Wq.bias', 'h.32.multi_head_attention.Wq.weight', 'h.32.multi_head_attention.Wv.bias', 'h.32.multi_head_attention.Wv.weight', 'h.32.multi_head_attention.dense.bias', 'h.32.multi_head_attention.dense.weight', 'h.33.ffn.0.bias', 'h.33.ffn.0.weight', 'h.33.ffn.2.bias', 'h.33.ffn.2.weight', 'h.33.layernorm1.bias', 'h.33.layernorm1.weight', 'h.33.layernorm2.bias', 'h.33.layernorm2.weight', 'h.33.multi_head_attention.Wk.bias', 'h.33.multi_head_attention.Wk.weight', 'h.33.multi_head_attention.Wq.bias', 'h.33.multi_head_attention.Wq.weight', 'h.33.multi_head_attention.Wv.bias', 'h.33.multi_head_attention.Wv.weight', 'h.33.multi_head_attention.dense.bias', 'h.33.multi_head_attention.dense.weight', 'h.34.ffn.0.bias', 'h.34.ffn.0.weight', 'h.34.ffn.2.bias', 'h.34.ffn.2.weight', 'h.34.layernorm1.bias', 'h.34.layernorm1.weight', 'h.34.layernorm2.bias', 'h.34.layernorm2.weight', 'h.34.multi_head_attention.Wk.bias', 'h.34.multi_head_attention.Wk.weight', 'h.34.multi_head_attention.Wq.bias', 'h.34.multi_head_attention.Wq.weight', 'h.34.multi_head_attention.Wv.bias', 'h.34.multi_head_attention.Wv.weight', 'h.34.multi_head_attention.dense.bias', 'h.34.multi_head_attention.dense.weight', 'h.35.ffn.0.bias', 'h.35.ffn.0.weight', 'h.35.ffn.2.bias', 'h.35.ffn.2.weight', 'h.35.layernorm1.bias', 'h.35.layernorm1.weight', 'h.35.layernorm2.bias', 'h.35.layernorm2.weight', 'h.35.multi_head_attention.Wk.bias', 'h.35.multi_head_attention.Wk.weight', 'h.35.multi_head_attention.Wq.bias', 'h.35.multi_head_attention.Wq.weight', 'h.35.multi_head_attention.Wv.bias', 'h.35.multi_head_attention.Wv.weight', 'h.35.multi_head_attention.dense.bias', 'h.35.multi_head_attention.dense.weight', 'h.36.ffn.0.bias', 'h.36.ffn.0.weight', 'h.36.ffn.2.bias', 'h.36.ffn.2.weight', 'h.36.layernorm1.bias', 'h.36.layernorm1.weight', 'h.36.layernorm2.bias', 'h.36.layernorm2.weight', 'h.36.multi_head_attention.Wk.bias', 'h.36.multi_head_attention.Wk.weight', 'h.36.multi_head_attention.Wq.bias', 'h.36.multi_head_attention.Wq.weight', 'h.36.multi_head_attention.Wv.bias', 'h.36.multi_head_attention.Wv.weight', 'h.36.multi_head_attention.dense.bias', 'h.36.multi_head_attention.dense.weight', 'h.37.ffn.0.bias', 'h.37.ffn.0.weight', 'h.37.ffn.2.bias', 'h.37.ffn.2.weight', 'h.37.layernorm1.bias', 'h.37.layernorm1.weight', 'h.37.layernorm2.bias', 'h.37.layernorm2.weight', 'h.37.multi_head_attention.Wk.bias', 'h.37.multi_head_attention.Wk.weight', 'h.37.multi_head_attention.Wq.bias', 'h.37.multi_head_attention.Wq.weight', 'h.37.multi_head_attention.Wv.bias', 'h.37.multi_head_attention.Wv.weight', 'h.37.multi_head_attention.dense.bias', 'h.37.multi_head_attention.dense.weight', 'h.38.ffn.0.bias', 'h.38.ffn.0.weight', 'h.38.ffn.2.bias', 'h.38.ffn.2.weight', 'h.38.layernorm1.bias', 'h.38.layernorm1.weight', 'h.38.layernorm2.bias', 'h.38.layernorm2.weight', 'h.38.multi_head_attention.Wk.bias', 'h.38.multi_head_attention.Wk.weight', 'h.38.multi_head_attention.Wq.bias', 'h.38.multi_head_attention.Wq.weight', 'h.38.multi_head_attention.Wv.bias', 'h.38.multi_head_attention.Wv.weight', 'h.38.multi_head_attention.dense.bias', 'h.38.multi_head_attention.dense.weight', 'h.39.ffn.0.bias', 'h.39.ffn.0.weight', 'h.39.ffn.2.bias', 'h.39.ffn.2.weight', 'h.39.layernorm1.bias', 'h.39.layernorm1.weight', 'h.39.layernorm2.bias', 'h.39.layernorm2.weight', 'h.39.multi_head_attention.Wk.bias', 'h.39.multi_head_attention.Wk.weight', 'h.39.multi_head_attention.Wq.bias', 'h.39.multi_head_attention.Wq.weight', 'h.39.multi_head_attention.Wv.bias', 'h.39.multi_head_attention.Wv.weight', 'h.39.multi_head_attention.dense.bias', 'h.39.multi_head_attention.dense.weight', 'h.4.ffn.0.bias', 'h.4.ffn.0.weight', 'h.4.ffn.2.bias', 'h.4.ffn.2.weight', 'h.4.layernorm1.bias', 'h.4.layernorm1.weight', 'h.4.layernorm2.bias', 'h.4.layernorm2.weight', 'h.4.multi_head_attention.Wk.bias', 'h.4.multi_head_attention.Wk.weight', 'h.4.multi_head_attention.Wq.bias', 'h.4.multi_head_attention.Wq.weight', 'h.4.multi_head_attention.Wv.bias', 'h.4.multi_head_attention.Wv.weight', 'h.4.multi_head_attention.dense.bias', 'h.4.multi_head_attention.dense.weight', 'h.40.ffn.0.bias', 'h.40.ffn.0.weight', 'h.40.ffn.2.bias', 'h.40.ffn.2.weight', 'h.40.layernorm1.bias', 'h.40.layernorm1.weight', 'h.40.layernorm2.bias', 'h.40.layernorm2.weight', 'h.40.multi_head_attention.Wk.bias', 'h.40.multi_head_attention.Wk.weight', 'h.40.multi_head_attention.Wq.bias', 'h.40.multi_head_attention.Wq.weight', 'h.40.multi_head_attention.Wv.bias', 'h.40.multi_head_attention.Wv.weight', 'h.40.multi_head_attention.dense.bias', 'h.40.multi_head_attention.dense.weight', 'h.41.ffn.0.bias', 'h.41.ffn.0.weight', 'h.41.ffn.2.bias', 'h.41.ffn.2.weight', 'h.41.layernorm1.bias', 'h.41.layernorm1.weight', 'h.41.layernorm2.bias', 'h.41.layernorm2.weight', 'h.41.multi_head_attention.Wk.bias', 'h.41.multi_head_attention.Wk.weight', 'h.41.multi_head_attention.Wq.bias', 'h.41.multi_head_attention.Wq.weight', 'h.41.multi_head_attention.Wv.bias', 'h.41.multi_head_attention.Wv.weight', 'h.41.multi_head_attention.dense.bias', 'h.41.multi_head_attention.dense.weight', 'h.42.ffn.0.bias', 'h.42.ffn.0.weight', 'h.42.ffn.2.bias', 'h.42.ffn.2.weight', 'h.42.layernorm1.bias', 'h.42.layernorm1.weight', 'h.42.layernorm2.bias', 'h.42.layernorm2.weight', 'h.42.multi_head_attention.Wk.bias', 'h.42.multi_head_attention.Wk.weight', 'h.42.multi_head_attention.Wq.bias', 'h.42.multi_head_attention.Wq.weight', 'h.42.multi_head_attention.Wv.bias', 'h.42.multi_head_attention.Wv.weight', 'h.42.multi_head_attention.dense.bias', 'h.42.multi_head_attention.dense.weight', 'h.43.ffn.0.bias', 'h.43.ffn.0.weight', 'h.43.ffn.2.bias', 'h.43.ffn.2.weight', 'h.43.layernorm1.bias', 'h.43.layernorm1.weight', 'h.43.layernorm2.bias', 'h.43.layernorm2.weight', 'h.43.multi_head_attention.Wk.bias', 'h.43.multi_head_attention.Wk.weight', 'h.43.multi_head_attention.Wq.bias', 'h.43.multi_head_attention.Wq.weight', 'h.43.multi_head_attention.Wv.bias', 'h.43.multi_head_attention.Wv.weight', 'h.43.multi_head_attention.dense.bias', 'h.43.multi_head_attention.dense.weight', 'h.44.ffn.0.bias', 'h.44.ffn.0.weight', 'h.44.ffn.2.bias', 'h.44.ffn.2.weight', 'h.44.layernorm1.bias', 'h.44.layernorm1.weight', 'h.44.layernorm2.bias', 'h.44.layernorm2.weight', 'h.44.multi_head_attention.Wk.bias', 'h.44.multi_head_attention.Wk.weight', 'h.44.multi_head_attention.Wq.bias', 'h.44.multi_head_attention.Wq.weight', 'h.44.multi_head_attention.Wv.bias', 'h.44.multi_head_attention.Wv.weight', 'h.44.multi_head_attention.dense.bias', 'h.44.multi_head_attention.dense.weight', 'h.45.ffn.0.bias', 'h.45.ffn.0.weight', 'h.45.ffn.2.bias', 'h.45.ffn.2.weight', 'h.45.layernorm1.bias', 'h.45.layernorm1.weight', 'h.45.layernorm2.bias', 'h.45.layernorm2.weight', 'h.45.multi_head_attention.Wk.bias', 'h.45.multi_head_attention.Wk.weight', 'h.45.multi_head_attention.Wq.bias', 'h.45.multi_head_attention.Wq.weight', 'h.45.multi_head_attention.Wv.bias', 'h.45.multi_head_attention.Wv.weight', 'h.45.multi_head_attention.dense.bias', 'h.45.multi_head_attention.dense.weight', 'h.46.ffn.0.bias', 'h.46.ffn.0.weight', 'h.46.ffn.2.bias', 'h.46.ffn.2.weight', 'h.46.layernorm1.bias', 'h.46.layernorm1.weight', 'h.46.layernorm2.bias', 'h.46.layernorm2.weight', 'h.46.multi_head_attention.Wk.bias', 'h.46.multi_head_attention.Wk.weight', 'h.46.multi_head_attention.Wq.bias', 'h.46.multi_head_attention.Wq.weight', 'h.46.multi_head_attention.Wv.bias', 'h.46.multi_head_attention.Wv.weight', 'h.46.multi_head_attention.dense.bias', 'h.46.multi_head_attention.dense.weight', 'h.47.ffn.0.bias', 'h.47.ffn.0.weight', 'h.47.ffn.2.bias', 'h.47.ffn.2.weight', 'h.47.layernorm1.bias', 'h.47.layernorm1.weight', 'h.47.layernorm2.bias', 'h.47.layernorm2.weight', 'h.47.multi_head_attention.Wk.bias', 'h.47.multi_head_attention.Wk.weight', 'h.47.multi_head_attention.Wq.bias', 'h.47.multi_head_attention.Wq.weight', 'h.47.multi_head_attention.Wv.bias', 'h.47.multi_head_attention.Wv.weight', 'h.47.multi_head_attention.dense.bias', 'h.47.multi_head_attention.dense.weight', 'h.5.ffn.0.bias', 'h.5.ffn.0.weight', 'h.5.ffn.2.bias', 'h.5.ffn.2.weight', 'h.5.layernorm1.bias', 'h.5.layernorm1.weight', 'h.5.layernorm2.bias', 'h.5.layernorm2.weight', 'h.5.multi_head_attention.Wk.bias', 'h.5.multi_head_attention.Wk.weight', 'h.5.multi_head_attention.Wq.bias', 'h.5.multi_head_attention.Wq.weight', 'h.5.multi_head_attention.Wv.bias', 'h.5.multi_head_attention.Wv.weight', 'h.5.multi_head_attention.dense.bias', 'h.5.multi_head_attention.dense.weight', 'h.6.ffn.0.bias', 'h.6.ffn.0.weight', 'h.6.ffn.2.bias', 'h.6.ffn.2.weight', 'h.6.layernorm1.bias', 'h.6.layernorm1.weight', 'h.6.layernorm2.bias', 'h.6.layernorm2.weight', 'h.6.multi_head_attention.Wk.bias', 'h.6.multi_head_attention.Wk.weight', 'h.6.multi_head_attention.Wq.bias', 'h.6.multi_head_attention.Wq.weight', 'h.6.multi_head_attention.Wv.bias', 'h.6.multi_head_attention.Wv.weight', 'h.6.multi_head_attention.dense.bias', 'h.6.multi_head_attention.dense.weight', 'h.7.ffn.0.bias', 'h.7.ffn.0.weight', 'h.7.ffn.2.bias', 'h.7.ffn.2.weight', 'h.7.layernorm1.bias', 'h.7.layernorm1.weight', 'h.7.layernorm2.bias', 'h.7.layernorm2.weight', 'h.7.multi_head_attention.Wk.bias', 'h.7.multi_head_attention.Wk.weight', 'h.7.multi_head_attention.Wq.bias', 'h.7.multi_head_attention.Wq.weight', 'h.7.multi_head_attention.Wv.bias', 'h.7.multi_head_attention.Wv.weight', 'h.7.multi_head_attention.dense.bias', 'h.7.multi_head_attention.dense.weight', 'h.8.ffn.0.bias', 'h.8.ffn.0.weight', 'h.8.ffn.2.bias', 'h.8.ffn.2.weight', 'h.8.layernorm1.bias', 'h.8.layernorm1.weight', 'h.8.layernorm2.bias', 'h.8.layernorm2.weight', 'h.8.multi_head_attention.Wk.bias', 'h.8.multi_head_attention.Wk.weight', 'h.8.multi_head_attention.Wq.bias', 'h.8.multi_head_attention.Wq.weight', 'h.8.multi_head_attention.Wv.bias', 'h.8.multi_head_attention.Wv.weight', 'h.8.multi_head_attention.dense.bias', 'h.8.multi_head_attention.dense.weight', 'h.9.ffn.0.bias', 'h.9.ffn.0.weight', 'h.9.ffn.2.bias', 'h.9.ffn.2.weight', 'h.9.layernorm1.bias', 'h.9.layernorm1.weight', 'h.9.layernorm2.bias', 'h.9.layernorm2.weight', 'h.9.multi_head_attention.Wk.bias', 'h.9.multi_head_attention.Wk.weight', 'h.9.multi_head_attention.Wq.bias', 'h.9.multi_head_attention.Wq.weight', 'h.9.multi_head_attention.Wv.bias', 'h.9.multi_head_attention.Wv.weight', 'h.9.multi_head_attention.dense.bias', 'h.9.multi_head_attention.dense.weight', 'layernorm.bias', 'layernorm.weight', 'w.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from transformers import CTRLForSequenceClassification\n",
    "\n",
    "labels = ['n', 'y']\n",
    "id2label = {i: label for i, label in enumerate(labels)}\n",
    "label2id = {label: i for i, label in id2label.items()}\n",
    "\n",
    "print('id2label:', id2label)\n",
    "print('label2id:', label2id)\n",
    "\n",
    "model = CTRLForSequenceClassification.from_pretrained(MODEL_NAME, ignore_mismatched_sizes=True, num_labels=len(labels), id2label=id2label, label2id=label2id)\n",
    "\n",
    "# Only train last classifier layer\n",
    "# for param in model.base_model.parameters():\n",
    "#     param.requires_grad = False\n",
    "# resize model embedding to match new tokenizer\n",
    "# model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# # fix model padding token id\n",
    "# model.config.pad_token_id = model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "mighty-address",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CTRLForSequenceClassification(\n",
       "  (transformer): CTRLModel(\n",
       "    (w): Embedding(32128, 1280)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-47): 48 x EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=1280, out_features=2, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "separated-purple",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Freeze all layers except the last one\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# for param in model.bert.pooler.dense.parameters():\n",
    "#     param.requires_grad = True\n",
    "\n",
    "# # Unfreeze the last three layers\n",
    "# for param in model.transformer.ln_f.parameters():\n",
    "#     param.requires_grad = True\n",
    "\n",
    "for param in model.parameters():\n",
    "    print(param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "exciting-bread",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1363280896"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bronze-amplifier",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2560"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protective-senior",
   "metadata": {},
   "source": [
    "##  Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "formal-scene",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='my_best_model',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "talented-bermuda",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/csci5541/zhan8023/.conda/envs/pytorch_jupyter_a40/lib/python3.12/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='660' max='660' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [660/660 17:58, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.706951</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.537313</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.672897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.698585</td>\n",
       "      <td>0.544444</td>\n",
       "      <td>0.544529</td>\n",
       "      <td>0.891667</td>\n",
       "      <td>0.676145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.694433</td>\n",
       "      <td>0.546667</td>\n",
       "      <td>0.546154</td>\n",
       "      <td>0.887500</td>\n",
       "      <td>0.676190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.692667</td>\n",
       "      <td>0.548889</td>\n",
       "      <td>0.547315</td>\n",
       "      <td>0.891667</td>\n",
       "      <td>0.678288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.691918</td>\n",
       "      <td>0.542222</td>\n",
       "      <td>0.543590</td>\n",
       "      <td>0.883333</td>\n",
       "      <td>0.673016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.687324</td>\n",
       "      <td>0.542222</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.664495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.690440</td>\n",
       "      <td>0.540000</td>\n",
       "      <td>0.542636</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.669856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.722800</td>\n",
       "      <td>0.685457</td>\n",
       "      <td>0.548889</td>\n",
       "      <td>0.550409</td>\n",
       "      <td>0.841667</td>\n",
       "      <td>0.665568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.722800</td>\n",
       "      <td>0.685279</td>\n",
       "      <td>0.548889</td>\n",
       "      <td>0.550409</td>\n",
       "      <td>0.841667</td>\n",
       "      <td>0.665568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.722800</td>\n",
       "      <td>0.685112</td>\n",
       "      <td>0.548889</td>\n",
       "      <td>0.550409</td>\n",
       "      <td>0.841667</td>\n",
       "      <td>0.665568</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the best model at my_best_model/checkpoint-660/pytorch_model.bin, if you are running a distributed training on multiple nodes, you should activate `--save_on_each_node`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=660, training_loss=0.7180439226555102, metrics={'train_runtime': 1108.113, 'train_samples_per_second': 18.951, 'train_steps_per_second': 0.596, 'total_flos': 2.680753818189005e+16, 'train_loss': 0.7180439226555102, 'epoch': 10.0})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data['data'],\n",
    "    eval_dataset=val_data['data'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "extended-surface",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='96' max='66' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [66/66 01:22]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.6789799332618713,\n",
       " 'eval_accuracy': 0.5652380952380952,\n",
       " 'eval_precision': 0.5636257989540965,\n",
       " 'eval_recall': 0.8568904593639576,\n",
       " 'eval_f1': 0.6799859796705222,\n",
       " 'eval_runtime': 60.4345,\n",
       " 'eval_samples_per_second': 34.748,\n",
       " 'eval_steps_per_second': 1.092,\n",
       " 'epoch': 10.0}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(train_data['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "explicit-needle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.6851117610931396,\n",
       " 'eval_accuracy': 0.5488888888888889,\n",
       " 'eval_precision': 0.5504087193460491,\n",
       " 'eval_recall': 0.8416666666666667,\n",
       " 'eval_f1': 0.6655683690280065,\n",
       " 'eval_runtime': 11.9094,\n",
       " 'eval_samples_per_second': 37.785,\n",
       " 'eval_steps_per_second': 1.26,\n",
       " 'epoch': 10.0}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(val_data['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "physical-occupation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.6999768018722534,\n",
       " 'eval_accuracy': 0.5311111111111111,\n",
       " 'eval_precision': 0.5295698924731183,\n",
       " 'eval_recall': 0.8454935622317596,\n",
       " 'eval_f1': 0.6512396694214876,\n",
       " 'eval_runtime': 11.0532,\n",
       " 'eval_samples_per_second': 40.712,\n",
       " 'eval_steps_per_second': 1.357,\n",
       " 'epoch': 10.0}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(test_data['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "passive-stamp",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "stuck-interval",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!rm -r my_best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerous-montreal",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_jupyter_a40",
   "language": "python",
   "name": "pytorch_jupyter_a40"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
